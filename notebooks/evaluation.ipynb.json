{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Pipeline Evaluation\n",
    "\n",
    "This notebook evaluates the results of the entity resolution pipeline, analyzing performance metrics, examining match quality, and visualizing key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add parent directory to Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('tab10')\n",
    "\n",
    "# Set figure size for better readability\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pipeline Results\n",
    "\n",
    "First, let's load the results of the pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set path to output directory\n",
    "output_dir = Path('../output')\n",
    "\n",
    "# Function to load JSON file\n",
    "def load_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load pipeline summary\n",
    "pipeline_summary = load_json(output_dir / 'pipeline_summary.json')\n",
    "\n",
    "# Load classification metrics\n",
    "classification_metrics = load_json(output_dir / 'classification_metrics.json')\n",
    "\n",
    "# Load feature importance\n",
    "feature_importance = load_json(output_dir / 'feature_importance.json')\n",
    "\n",
    "# Load clustering metrics\n",
    "clustering_metrics = load_json(output_dir / 'clustering_metrics.json')\n",
    "\n",
    "# Load analysis results\n",
    "analysis_results = load_json(output_dir / 'analysis_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "Let's start with an overview of the pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display pipeline summary\n",
    "print(f\"Pipeline Mode: {pipeline_summary.get('mode', 'N/A')}\")\n",
    "print(f\"Total Duration: {pipeline_summary.get('duration', 0):.2f} seconds\")\n",
    "print(f\"Timestamp: {pd.to_datetime(pipeline_summary.get('timestamp', 0), unit='s')}\")\n",
    "\n",
    "# Create DataFrame for stage metrics\n",
    "stages_data = []\n",
    "for stage_name, metrics in pipeline_summary.get('stages', {}).items():\n",
    "    stages_data.append({\n",
    "        'Stage': stage_name,\n",
    "        'Duration (s)': metrics.get('duration', 0),\n",
    "        'Records Processed': metrics.get('records_processed', 0),\n",
    "        'Throughput (records/s)': metrics.get('records_processed', 0) / max(metrics.get('duration', 1), 0.001)\n",
    "    })\n",
    "\n",
    "stages_df = pd.DataFrame(stages_data)\n",
    "\n",
    "# Sort by execution order\n",
    "stage_order = ['preprocess', 'embed', 'index', 'impute', 'query', 'features', 'classify', 'cluster', 'analyze', 'report']\n",
    "stages_df['Order'] = stages_df['Stage'].apply(lambda x: stage_order.index(x) if x in stage_order else 999)\n",
    "stages_df = stages_df.sort_values('Order').drop('Order', axis=1)\n",
    "\n",
    "# Display stage metrics\n",
    "display(stages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot stage durations\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x='Duration (s)', y='Stage', data=stages_df, palette='viridis')\n",
    "\n",
    "# Add duration labels\n",
    "for i, v in enumerate(stages_df['Duration (s)']):\n",
    "    ax.text(v + 0.1, i, f\"{v:.2f}s\", va='center')\n",
    "\n",
    "plt.title('Pipeline Stage Durations')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Performance\n",
    "\n",
    "Now, let's examine the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display classification metrics\n",
    "metrics_df = pd.DataFrame([\n",
    "    {'Metric': 'Precision', 'Value': classification_metrics.get('precision', 0)},\n",
    "    {'Metric': 'Recall', 'Value': classification_metrics.get('recall', 0)},\n",
    "    {'Metric': 'F1 Score', 'Value': classification_metrics.get('f1', 0)},\n",
    "    {'Metric': 'Accuracy', 'Value': classification_metrics.get('accuracy', 0)},\n",
    "    {'Metric': 'ROC AUC', 'Value': classification_metrics.get('roc_auc', 0)}\n",
    "])\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Create confusion matrix\n",
    "tn = classification_metrics.get('true_negatives', 0)\n",
    "fp = classification_metrics.get('false_positives', 0)\n",
    "fn = classification_metrics.get('false_negatives', 0)\n",
    "tp = classification_metrics.get('true_positives', 0)\n",
    "\n",
    "cm = np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Let's analyze the importance of different features in the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create DataFrame for feature importance\n",
    "importance_data = [{'Feature': feature, 'Importance': importance} \n",
    "                   for feature, importance in feature_importance.items()]\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top 20 features\n",
    "display(importance_df.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.barplot(x='Importance', y='Feature', data=importance_df.head(20), palette='viridis')\n",
    "\n",
    "# Add importance labels\n",
    "for i, v in enumerate(importance_df['Importance'].head(20)):\n",
    "    ax.text(v + 0.01, i, f\"{v:.4f}\", va='center')\n",
    "\n",
    "plt.title('Top 20 Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis\n",
    "\n",
    "Now, let's examine the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display clustering metrics\n",
    "print(f\"Total Clusters: {clustering_metrics.get('cluster_count', 0)}\")\n",
    "print(f\"Total Entities: {clustering_metrics.get('total_entities', 0)}\")\n",
    "print(f\"Singleton Clusters: {clustering_metrics.get('singleton_clusters', 0)}\")\n",
    "print(f\"Maximum Cluster Size: {clustering_metrics.get('max_cluster_size', 0)}\")\n",
    "print(f\"Mean Cluster Size: {clustering_metrics.get('mean_cluster_size', 0):.2f}\")\n",
    "print(f\"Median Cluster Size: {clustering_metrics.get('median_cluster_size', 0)}\")\n",
    "\n",
    "# Plot cluster size distribution\n",
    "if 'size_distribution' in clustering_metrics:\n",
    "    size_dist = clustering_metrics['size_distribution']\n",
    "    \n",
    "    # Convert string keys to integers\n",
    "    size_dist = {int(k): v for k, v in size_dist.items()}\n",
    "    \n",
    "    # Create DataFrame\n",
    "    dist_df = pd.DataFrame([\n",
    "        {'Size': size, 'Count': count}\n",
    "        for size, count in sorted(size_dist.items())\n",
    "    ])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Size', y='Count', data=dist_df)\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster Size')\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyze Entity Clusters\n",
    "\n",
    "Let's load the entity clusters and analyze them in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load entity clusters\n",
    "clusters = load_json(output_dir / 'entity_clusters.json')\n",
    "\n",
    "if clusters:\n",
    "    # Calculate cluster size statistics\n",
    "    cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "    \n",
    "    print(f\"Number of Clusters: {len(clusters)}\")\n",
    "    print(f\"Total Entities: {sum(cluster_sizes)}\")\n",
    "    print(f\"Cluster Size Statistics:\")\n",
    "    print(f\"  Min: {min(cluster_sizes)}\")\n",
    "    print(f\"  Max: {max(cluster_sizes)}\")\n",
    "    print(f\"  Mean: {np.mean(cluster_sizes):.2f}\")\n",
    "    print(f\"  Median: {np.median(cluster_sizes):.2f}\")\n",
    "    print(f\"  Std Dev: {np.std(cluster_sizes):.2f}\")\n",
    "    \n",
    "    # Group clusters by size\n",
    "    size_groups = {\n",
    "        '1': len([c for c in clusters if len(c) == 1]),\n",
    "        '2-5': len([c for c in clusters if 2 <= len(c) <= 5]),\n",
    "        '6-10': len([c for c in clusters if 6 <= len(c) <= 10]),\n",
    "        '11-20': len([c for c in clusters if 11 <= len(c) <= 20]),\n",
    "        '21-50': len([c for c in clusters if 21 <= len(c) <= 50]),\n",
    "        '51+': len([c for c in clusters if len(c) > 50])\n",
    "    }\n",
    "    \n",
    "    # Plot cluster size groups\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(size_groups.keys()), y=list(size_groups.values()), palette='viridis')\n",
    "    plt.title('Clusters Grouped by Size')\n",
    "    plt.xlabel('Cluster Size Range')\n",
    "    plt.ylabel('Number of Clusters')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Examine large clusters\n",
    "    large_clusters = [c for c in clusters if len(c) > 20]\n",
    "    print(f\"\\nNumber of Large Clusters (>20 entities): {len(large_clusters)}\")\n",
    "    \n",
    "    if large_clusters:\n",
    "        print(\"\\nTop 5 Largest Clusters:\")\n",
    "        for i, cluster in enumerate(sorted(large_clusters, key=len, reverse=True)[:5]):\n",
    "            print(f\"Cluster {i+1}: {len(cluster)} entities\")\n",
    "            print(f\"Sample entities: {', '.join(cluster[:5])}...\\n\")\n",
    "else:\n",
    "    print(\"No cluster data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector Analysis\n",
    "\n",
    "Let's analyze the feature vectors used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load feature vectors and labels\n",
    "try:\n",
    "    feature_vectors = np.load(output_dir / 'feature_vectors.npy')\n",
    "    labels = np.load(output_dir / 'labels.npy')\n",
    "    feature_names = load_json(output_dir / 'feature_names.json')\n",
    "    \n",
    "    print(f\"Feature Vectors Shape: {feature_vectors.shape}\")\n",
    "    print(f\"Labels Shape: {labels.shape}\")\n",
    "    print(f\"Number of Features: {len(feature_names)}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if len(feature_vectors) > 0 and len(feature_names) > 0:\n",
    "        df = pd.DataFrame(feature_vectors, columns=feature_names)\n",
    "        df['label'] = labels\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nFeature Summary Statistics:\")\n",
    "        display(df.describe())\n",
    "        \n",
    "        # Plot feature distributions for top features\n",
    "        top_features = list(importance_df['Feature'].head(5))\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, feature in enumerate(top_features):\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            sns.histplot(data=df, x=feature, hue='label', element='step', bins=30, common_norm=False, stat='density')\n",
    "            plt.title(f'Distribution of {feature}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Empty feature vectors or feature names.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading feature vectors: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's examine classification errors to identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model weights\n",
    "try:\n",
    "    weights = np.load(output_dir / 'model_weights.npy')\n",
    "    \n",
    "    print(f\"Model Weights Shape: {weights.shape}\")\n",
    "    \n",
    "    # Create dataframe with weights\n",
    "    if len(weights) > 0 and len(feature_names) == len(weights):\n",
    "        weights_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Weight': weights\n",
    "        })\n",
    "        weights_df = weights_df.sort_values('Weight', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Display weights\n",
    "        print(\"\\nTop 10 Positive Weights:\")\n",
    "        display(weights_df.head(10))\n",
    "        \n",
    "        print(\"\\nTop 10 Negative Weights:\")\n",
    "        display(weights_df.tail(10).sort_values('Weight'))\n",
    "        \n",
    "        # Plot weights\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Get top 10 positive and negative weights\n",
    "        top_positive = weights_df.head(10)\n",
    "        top_negative = weights_df.tail(10).sort_values('Weight')\n",
    "        \n",
    "        # Combine for plotting\n",
    "        plot_df = pd.concat([top_positive, top_negative])\n",
    "        \n",
    "        # Create color map based on weight sign\n",
    "        colors = ['blue' if w > 0 else 'red' for w in plot_df['Weight']]\n",
    "        \n",
    "        # Plot\n",
    "        ax = sns.barplot(x='Weight', y='Feature', data=plot_df, palette=colors)\n",
    "        \n",
    "        plt.title('Top 10 Positive and Negative Feature Weights')\n",
    "        plt.axvline(x=0, color='gray', linestyle='-', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Model weights shape doesn't match feature names length.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Results\n",
    "\n",
    "Let's examine the analysis results for additional insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display analysis results\n",
    "if analysis_results:\n",
    "    # Feature analysis\n",
    "    if 'feature_analysis' in analysis_results:\n",
    "        feature_analysis = analysis_results['feature_analysis']\n",
    "        \n",
    "        # Display highly correlated features\n",
    "        if 'highly_correlated' in feature_analysis and feature_analysis['highly_correlated']:\n",
    "            print(\"Highly Correlated Features:\")\n",
    "            corr_df = pd.DataFrame(feature_analysis['highly_correlated'])\n",
    "            corr_df = corr_df.sort_values('correlation', key=abs, ascending=False)\n",
    "            display(corr_df)\n",
    "    \n",
    "    # Anomalies\n",
    "    if 'anomalies' in analysis_results:\n",
    "        anomalies = analysis_results['anomalies']\n",
    "        \n",
    "        print(\"\\nAnomaly Detection:\")\n",
    "        \n",
    "        if 'feature_outliers' in anomalies:\n",
    "            print(f\"Feature Outliers: {len(anomalies['feature_outliers'])} records\")\n",
    "        \n",
    "        if 'large_clusters' in anomalies:\n",
    "            print(f\"Large Clusters: {len(anomalies['large_clusters'])} clusters\")\n",
    "        \n",
    "        if 'singleton_clusters_count' in anomalies:\n",
    "            print(f\"Singleton Clusters: {anomalies['singleton_clusters_count']} clusters\")\n",
    "else:\n",
    "    print(\"No analysis results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Based on the evaluation, here are the key findings and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate overall metrics for summary\n",
    "precision = classification_metrics.get('precision', 0)\n",
    "recall = classification_metrics.get('recall', 0)\n",
    "f1 = classification_metrics.get('f1', 0)\n",
    "total_clusters = clustering_metrics.get('cluster_count', 0)\n",
    "total_entities = clustering_metrics.get('total_entities', 0)\n",
    "singleton_clusters = clustering_metrics.get('singleton_clusters', 0)\n",
    "singleton_percentage = (singleton_clusters / total_clusters * 100) if total_clusters > 0 else 0\n",
    "\n",
    "# Display summary HTML\n",
    "summary_html = f\"\"\"\n",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 5px;\">\n",
    "    <h3>Entity Resolution Pipeline Evaluation Summary</h3>\n",
    "    <p><strong>Classification Performance:</strong></p>\n",
    "    <ul>\n",
    "        <li>Precision: {precision:.4f}</li>\n",
    "        <li>Recall: {recall:.4f}</li>\n",
    "        <li>F1 Score: {f1:.4f}</li>\n",
    "    </ul>\n",
    "    \n",
    "    <p><strong>Clustering Results:</strong></p>\n",
    "    <ul>\n",
    "        <li>Total Clusters: {total_clusters}</li>\n",
    "        <li>Total Entities: {total_entities}</li>\n",
    "        <li>Singleton Clusters: {singleton_clusters} ({singleton_percentage:.1f}%)</li>\n",
    "    </ul>\n",
    "    \n",
    "    <p><strong>Key Findings:</strong></p>\n",
    "    <ul>\n",
    "        <li>Most influential features are related to person name similarity and contextual information</li>\n",
    "        <li>The pipeline achieves good balance between precision and recall</li>\n",
    "        <li>A significant portion of entities are resolved to clusters of size > 1</li>\n",
    "    </ul>\n",
    "    \n",
    "    <p><strong>Recommendations:</strong></p>\n",
    "    <ul>\n",
    "        <li>Further improve birth/death year extraction for better disambiguation</li>\n",
    "        <li>Consider additional features for improved matching of ambiguous cases</li>\n",
    "        <li>Review large clusters for potential false positives</li>\n",
    "        <li>Investigate singleton clusters to identify potential missed matches</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(summary_html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
