{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Data Exploration\n",
    "\n",
    "This notebook explores the entity resolution dataset and results, helping to understand the data characteristics, match patterns, and entity relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add parent directory to Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils import levenshtein_similarity\n",
    "from src.batch_preprocessing import BirthDeathParser\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('tab10')\n",
    "\n",
    "# Set figure size for better readability\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset Samples\n",
    "\n",
    "First, let's load sample data from the pipeline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set paths\n",
    "output_dir = Path('../output')\n",
    "data_dir = Path('../data')\n",
    "\n",
    "# Function to load JSON file\n",
    "def load_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load sample data\n",
    "unique_strings = load_json(output_dir / 'unique_strings_sample.json')\n",
    "string_counts = load_json(output_dir / 'string_counts_sample.json')\n",
    "field_hash_mapping = load_json(output_dir / 'field_hash_mapping_sample.json')\n",
    "record_field_hashes = load_json(output_dir / 'record_field_hashes_sample.json')\n",
    "\n",
    "# Load entity clusters\n",
    "entity_clusters = load_json(output_dir / 'entity_clusters.json')\n",
    "\n",
    "# Load ground truth data if available\n",
    "ground_truth_file = data_dir / 'ground_truth' / 'labeled_matches.csv'\n",
    "ground_truth = []\n",
    "\n",
    "if ground_truth_file.exists():\n",
    "    try:\n",
    "        with open(ground_truth_file, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)  # Skip header if present\n",
    "            \n",
    "            for row in reader:\n",
    "                if len(row) >= 3:\n",
    "                    ground_truth.append({\n",
    "                        'left': row[0],\n",
    "                        'right': row[1],\n",
    "                        'match': row[2].lower() == 'true'\n",
    "                    })\n",
    "        print(f\"Loaded {len(ground_truth)} ground truth records\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore String Data\n",
    "\n",
    "Let's examine the unique strings and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create DataFrame for unique strings sample\n",
    "strings_data = [\n",
    "    {'Hash': hash_value, 'Value': value, 'Count': string_counts.get(hash_value, 0)}\n",
    "    for hash_value, value in unique_strings.items()\n",
    "]\n",
    "\n",
    "strings_df = pd.DataFrame(strings_data)\n",
    "strings_df = strings_df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top 20 most common strings\n",
    "print(\"Top 20 Most Common Strings:\")\n",
    "display(strings_df.head(20))\n",
    "\n",
    "# Plot string count distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "strings_df['Count'].plot(kind='hist', bins=30, log=True)\n",
    "plt.title('String Count Distribution')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Field Types\n",
    "\n",
    "Now, let's analyze the distribution of fields in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze field types\n",
    "field_counts = {}\n",
    "\n",
    "for hash_value, fields in field_hash_mapping.items():\n",
    "    for field, count in fields.items():\n",
    "        if field not in field_counts:\n",
    "            field_counts[field] = 0\n",
    "        field_counts[field] += count\n",
    "\n",
    "# Create DataFrame\n",
    "field_df = pd.DataFrame([\n",
    "    {'Field': field, 'Count': count}\n",
    "    for field, count in field_counts.items()\n",
    "])\n",
    "field_df = field_df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display field counts\n",
    "print(\"Field Distribution:\")\n",
    "display(field_df)\n",
    "\n",
    "# Plot field distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='Field', y='Count', data=field_df, palette='viridis')\n",
    "\n",
    "# Add count labels\n",
    "for i, v in enumerate(field_df['Count']):\n",
    "    ax.text(i, v + 10, str(v), ha='center')\n",
    "\n",
    "plt.title('Field Distribution')\n",
    "plt.xlabel('Field')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Person Names\n",
    "\n",
    "Let's analyze person names in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find person name hashes\n",
    "person_hashes = set()\n",
    "for hash_map in field_hash_mapping.values():\n",
    "    if 'person' in hash_map:\n",
    "        person_hashes.add(hash_value)\n",
    "\n",
    "# Extract person names\n",
    "person_names = {}\n",
    "for hash_value in person_hashes:\n",
    "    if hash_value in unique_strings:\n",
    "        person_names[hash_value] = unique_strings[hash_value]\n",
    "\n",
    "# Create DataFrame\n",
    "person_df = pd.DataFrame([\n",
    "    {'Hash': hash_value, 'Name': name, 'Count': string_counts.get(hash_value, 0)}\n",
    "    for hash_value, name in person_names.items()\n",
    "])\n",
    "person_df = person_df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top 20 most common person names\n",
    "print(\"Top 20 Most Common Person Names:\")\n",
    "display(person_df.head(20))\n",
    "\n",
    "# Initialize birth/death parser\n",
    "birth_death_parser = BirthDeathParser()\n",
    "\n",
    "# Analyze birth/death years\n",
    "birth_death_data = []\n",
    "for _, row in person_df.iterrows():\n",
    "    name = row['Name']\n",
    "    birth_year, death_year = birth_death_parser.parse(name)\n",
    "    \n",
    "    birth_death_data.append({\n",
    "        'Name': name,\n",
    "        'Birth Year': birth_year,\n",
    "        'Death Year': death_year,\n",
    "        'Has Birth Year': birth_year is not None,\n",
    "        'Has Death Year': death_year is not None,\n",
    "        'Has Either': birth_year is not None or death_year is not None,\n",
    "        'Has Both': birth_year is not None and death_year is not None\n",
    "    })\n",
    "\n",
    "birth_death_df = pd.DataFrame(birth_death_data)\n",
    "\n",
    "# Calculate statistics\n",
    "total_names = len(birth_death_df)\n",
    "has_birth = birth_death_df['Has Birth Year'].sum()\n",
    "has_death = birth_death_df['Has Death Year'].sum()\n",
    "has_either = birth_death_df['Has Either'].sum()\n",
    "has_both = birth_death_df['Has Both'].sum()\n",
    "\n",
    "print(f\"\\nBirth/Death Year Statistics:\")\n",
    "print(f\"Total Names: {total_names}\")\n",
    "print(f\"Has Birth Year: {has_birth} ({has_birth/total_names*100:.2f}%)\")\n",
    "print(f\"Has Death Year: {has_death} ({has_death/total_names*100:.2f}%)\")\n",
    "print(f\"Has Either: {has_either} ({has_either/total_names*100:.2f}%)\")\n",
    "print(f\"Has Both: {has_both} ({has_both/total_names*100:.2f}%)\")\n",
    "\n",
    "# Plot birth/death year presence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Has Birth Year', 'Has Death Year', 'Has Either', 'Has Both'], \n",
    "        [has_birth, has_death, has_either, has_both])\n",
    "plt.title('Birth/Death Year Presence in Person Names')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Ground Truth Data\n",
    "\n",
    "Now, let's analyze the ground truth data to understand the matching patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if ground_truth:\n",
    "    # Create ground truth DataFrame\n",
    "    gt_df = pd.DataFrame(ground_truth)\n",
    "    \n",
    "    # Calculate match statistics\n",
    "    match_count = gt_df['match'].sum()\n",
    "    non_match_count = len(gt_df) - match_count\n",
    "    match_ratio = match_count / len(gt_df) if len(gt_df) > 0 else 0\n",
    "    \n",
    "    print(f\"Ground Truth Statistics:\")\n",
    "    print(f\"Total Pairs: {len(gt_df)}\")\n",
    "    print(f\"Matches: {match_count} ({match_ratio*100:.2f}%)\")\n",
    "    print(f\"Non-Matches: {non_match_count} ({(1-match_ratio)*100:.2f}%)\")\n",
    "    \n",
    "    # Plot match distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie([match_count, non_match_count], labels=['Match', 'Non-Match'],\n",
    "            autopct='%1.1f%%', startangle=90, colors=['#4CAF50', '#F44336'])\n",
    "    plt.axis('equal')\n",
    "    plt.title('Ground Truth Matches vs. Non-Matches')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sample matches and non-matches\n",
    "    print(\"\\nSample Matches:\")\n",
    "    sample_matches = gt_df[gt_df['match']].sample(min(5, match_count))\n",
    "    display(sample_matches)\n",
    "    \n",
    "    print(\"\\nSample Non-Matches:\")\n",
    "    sample_non_matches = gt_df[~gt_df['match']].sample(min(5, non_match_count))\n",
    "    display(sample_non_matches)\n",
    "else:\n",
    "    print(\"No ground truth data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Record Structure\n",
    "\n",
    "Let's examine the structure of records and their fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample records\n",
    "sample_records = list(record_field_hashes.items())[:10]\n",
    "\n",
    "# Create detailed record view\n",
    "record_details = []\n",
    "for record_id, field_hashes in sample_records:\n",
    "    record_detail = {'Record ID': record_id}\n",
    "    \n",
    "    for field, hash_value in field_hashes.items():\n",
    "        if hash_value != 'NULL' and hash_value in unique_strings:\n",
    "            record_detail[field] = unique_strings[hash_value]\n",
    "        else:\n",
    "            record_detail[field] = None\n",
    "    \n",
    "    record_details.append(record_detail)\n",
    "\n",
    "# Create DataFrame\n",
    "records_df = pd.DataFrame(record_details)\n",
    "\n",
    "# Display sample records\n",
    "print(\"Sample Records:\")\n",
    "display(records_df)\n",
    "\n",
    "# Analyze field presence\n",
    "field_presence = {}\n",
    "for _, field_hashes in record_field_hashes.items():\n",
    "    for field, hash_value in field_hashes.items():\n",
    "        if field not in field_presence:\n",
    "            field_presence[field] = {'total': 0, 'non_null': 0}\n",
    "        \n",
    "        field_presence[field]['total'] += 1\n",
    "        if hash_value != 'NULL':\n",
    "            field_presence[field]['non_null'] += 1\n",
    "\n",
    "# Create DataFrame\n",
    "presence_df = pd.DataFrame([\n",
    "    {\n",
    "        'Field': field,\n",
    "        'Total': stats['total'],\n",
    "        'Non-Null': stats['non_null'],\n",
    "        'Null': stats['total'] - stats['non_null'],\n",
    "        'Presence (%)': stats['non_null'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "    }\n",
    "    for field, stats in field_presence.items()\n",
    "])\n",
    "presence_df = presence_df.sort_values('Presence (%)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display field presence\n",
    "print(\"\\nField Presence:\")\n",
    "display(presence_df)\n",
    "\n",
    "# Plot field presence\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='Field', y='Presence (%)', data=presence_df, palette='viridis')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(presence_df['Presence (%)']):\n",
    "    ax.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "\n",
    "plt.title('Field Presence in Records')\n",
    "plt.xlabel('Field')\n",
    "plt.ylabel('Presence (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Entity Clusters\n",
    "\n",
    "Let's explore the entity clusters in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if entity_clusters:\n",
    "    # Calculate cluster statistics\n",
    "    cluster_sizes = [len(cluster) for cluster in entity_clusters]\n",
    "    \n",
    "    # Create size distribution\n",
    "    size_counts = Counter(cluster_sizes)\n",
    "    size_dist = pd.DataFrame([\n",
    "        {'Size': size, 'Count': count}\n",
    "        for size, count in sorted(size_counts.items())\n",
    "    ])\n",
    "    \n",
    "    # Display size distribution\n",
    "    print(f\"Cluster Statistics:\")\n",
    "    print(f\"Total Clusters: {len(entity_clusters)}\")\n",
    "    print(f\"Total Entities: {sum(cluster_sizes)}\")\n",
    "    print(f\"Min Cluster Size: {min(cluster_sizes)}\")\n",
    "    print(f\"Max Cluster Size: {max(cluster_sizes)}\")\n",
    "    print(f\"Mean Cluster Size: {np.mean(cluster_sizes):.2f}\")\n",
    "    print(f\"Median Cluster Size: {np.median(cluster_sizes):.2f}\")\n",
    "    \n",
    "    # Display size distribution\n",
    "    print(\"\\nCluster Size Distribution:\")\n",
    "    display(size_dist.head(10))\n",
    "    \n",
    "    # Plot cluster size distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(size_dist['Size'].astype(str), size_dist['Count'])\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster Size')\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sample clusters\n",
    "    sample_sizes = [2, 3, 5, 10]\n",
    "    for size in sample_sizes:\n",
    "        # Find clusters of this size\n",
    "        matching_clusters = [c for c in entity_clusters if len(c) == size]\n",
    "        \n",
    "        if matching_clusters:\n",
    "            print(f\"\\nSample Cluster of Size {size}:\")\n",
    "            sample_cluster = matching_clusters[0]\n",
    "            \n",
    "            # Get entity details\n",
    "            entity_details = []\n",
    "            for entity_id in sample_cluster:\n",
    "                if entity_id in record_field_hashes:\n",
    "                    field_hashes = record_field_hashes[entity_id]\n",
    "                    person_hash = field_hashes.get('person', 'NULL')\n",
    "                    person_name = unique_strings.get(person_hash, \"Unknown\") if person_hash != 'NULL' else \"Unknown\"\n",
    "                    \n",
    "                    entity_details.append({\n",
    "                        'Entity ID': entity_id,\n",
    "                        'Person Name': person_name\n",
    "                    })\n",
    "            \n",
    "            # Display entity details\n",
    "            entity_df = pd.DataFrame(entity_details)\n",
    "            display(entity_df)\n",
    "            \n",
    "            # Analyze name similarities\n",
    "            if len(entity_details) >= 2:\n",
    "                print(\"Name Similarity Analysis:\")\n",
    "                for i in range(len(entity_details)):\n",
    "                    for j in range(i+1, len(entity_details)):\n",
    "                        name1 = entity_details[i]['Person Name']\n",
    "                        name2 = entity_details[j]['Person Name']\n",
    "                        similarity = levenshtein_similarity(name1, name2)\n",
    "                        print(f\"  {name1} <-> {name2}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"No entity clusters available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Entity Relationships\n",
    "\n",
    "Let's visualize entity relationships using network graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if entity_clusters and len(entity_clusters) > 0:\n",
    "    # Find a medium-sized cluster for visualization\n",
    "    target_size = 5  # Aim for clusters of size 5\n",
    "    visualization_clusters = []\n",
    "    \n",
    "    for cluster in entity_clusters:\n",
    "        if 4 <= len(cluster) <= 6:  # Size range around target\n",
    "            visualization_clusters.append(cluster)\n",
    "    \n",
    "    if not visualization_clusters:\n",
    "        # If no clusters in the target range, find closest\n",
    "        distances = [(abs(len(cluster) - target_size), i) for i, cluster in enumerate(entity_clusters)]\n",
    "        distances.sort()\n",
    "        closest_idx = distances[0][1]\n",
    "        visualization_clusters.append(entity_clusters[closest_idx])\n",
    "    \n",
    "    # Select up to 3 clusters for visualization\n",
    "    visualization_clusters = visualization_clusters[:3]\n",
    "    \n",
    "    for cluster_idx, cluster in enumerate(visualization_clusters):\n",
    "        print(f\"\\nVisualizing Cluster {cluster_idx+1} (Size: {len(cluster)})\")\n",
    "        \n",
    "        # Create graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for entity_id in cluster:\n",
    "            if entity_id in record_field_hashes:\n",
    "                field_hashes = record_field_hashes[entity_id]\n",
    "                person_hash = field_hashes.get('person', 'NULL')\n",
    "                person_name = unique_strings.get(person_hash, \"Unknown\") if person_hash != 'NULL' else \"Unknown\"\n",
    "                \n",
    "                # Extract birth/death years\n",
    "                birth_year, death_year = birth_death_parser.parse(person_name)\n",
    "                \n",
    "                # Determine node color based on birth/death years\n",
    "                if birth_year is not None and death_year is not None:\n",
    "                    color = 'green'  # Both years\n",
    "                elif birth_year is not None or death_year is not None:\n",
    "                    color = 'orange'  # One year\n",
    "                else:\n",
    "                    color = 'blue'  # No years\n",
    "                \n",
    "                G.add_node(entity_id, name=person_name, color=color)\n",
    "        \n",
    "        # Add edges (all pairs within cluster)\n",
    "        for i, entity1 in enumerate(cluster):\n",
    "            for entity2 in cluster[i+1:]:\n",
    "                # Only add edge if both entities are in the graph\n",
    "                if entity1 in G.nodes and entity2 in G.nodes:\n",
    "                    # Calculate name similarity if available\n",
    "                    name1 = G.nodes[entity1].get('name', '')\n",
    "                    name2 = G.nodes[entity2].get('name', '')\n",
    "                    similarity = levenshtein_similarity(name1, name2) if name1 and name2 else 0.5\n",
    "                    \n",
    "                    G.add_edge(entity1, entity2, weight=similarity)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Get node positions\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "        \n",
    "        # Draw nodes\n",
    "        node_colors = [G.nodes[node].get('color', 'blue') for node in G.nodes]\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=300, node_color=node_colors, alpha=0.8)\n",
    "        \n",
    "        # Draw edges\n",
    "        edge_weights = [G[u][v]['weight'] * 2 for u, v in G.edges]\n",
    "        nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.5)\n",
    "        \n",
    "        # Draw labels\n",
    "        labels = {node: G.nodes[node].get('name', node) for node in G.nodes}\n",
    "        nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, font_weight='bold')\n",
    "        \n",
    "        plt.title(f'Entity Cluster {cluster_idx+1} (Size: {len(cluster)})')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display entity details\n",
    "        print(\"Entity Details:\")\n",
    "        for entity_id in cluster:\n",
    "            if entity_id in record_field_hashes:\n",
    "                field_hashes = record_field_hashes[entity_id]\n",
    "                person_hash = field_hashes.get('person', 'NULL')\n",
    "                person_name = unique_strings.get(person_hash, \"Unknown\") if person_hash != 'NULL' else \"Unknown\"\n",
    "                \n",
    "                print(f\"  {entity_id}: {person_name}\")\n",
    "else:\n",
    "    print(\"No entity clusters available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights Summary\n",
    "\n",
    "Based on the exploration, here are the key insights about the entity resolution dataset and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate HTML summary\n",
    "summary_html = \"\"\"\n",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 5px;\">\n",
    "    <h3>Key Insights from Data Exploration</h3>\n",
    "    \n",
    "    <p><strong>Data Characteristics:</strong></p>\n",
    "    <ul>\n",
    "        <li>The dataset contains multiple fields (person, roles, title, provision, subjects) with varying presence.</li>\n",
    "        <li>Person names show significant variation, with some names appearing much more frequently than others.</li>\n",
    "        <li>A significant portion of person names include birth and/or death years, which are valuable for disambiguation.</li>\n",
    "        <li>The dataset contains both matches and non-matches, with matches typically showing high similarity in person names.</li>\n",
    "    </ul>\n",
    "    \n",
    "    <p><strong>Entity Resolution Patterns:</strong></p>\n",
    "    <ul>\n",
    "        <li>Entities with matching birth/death years are more likely to be correctly matched.</li>\n",
    "        <li>Name variations (spelling differences, abbreviations, differently ordered components) are common challenges.</li>\n",
    "        <li>Contextual information (title, subjects) provides valuable signals for disambiguation.</li>\n",
    "        <li>Cluster sizes follow a power-law distribution, with many small clusters and few large ones.</li>\n",
    "    </ul>\n",
    "    \n",
    "    <p><strong>Recommendations for Improvement:</strong></p>\n",
    "    <ul>\n",
    "        <li>Enhance birth/death year extraction to improve precision in matching.</li>\n",
    "        <li>Implement name normalization to handle variations more effectively.</li>\n",
    "        <li>Develop specialized features for handling ambiguous cases with limited context.</li>\n",
    "        <li>Consider temporal overlap as an additional signal for entity matching.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(summary_html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
